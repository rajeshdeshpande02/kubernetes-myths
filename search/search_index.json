{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kubernetes Myths","text":"<p>Kubernetes Myths is a deep dive into the most common misconceptions surrounding Kubernetes. From misunderstandings about networking, scheduling, and security to the myths that even experienced engineers fall for, this book breaks them down with real-world scenarios, technical evidence, and hands-on experiments.</p> <p>Whether you're a beginner trying to grasp Kubernetes fundamentals or a seasoned engineer looking to refine your expertise, this book challenges what you \"think\" you know. Each myth is dissected with a backstory, technical analysis, and a step-by-step validation\u2014so you don\u2019t just read, you prove it yourself.</p>"},{"location":"about/","title":"About","text":"<p>This is the about page. Work in progress</p>"},{"location":"architecture_myths/myth1/","title":"Myth 1: Kubelet is Exclusive to Worker Nodes","text":"<p>You SSH into a control plane node, expecting to see only control plane components like the API server, controller manager, and scheduler running. But wait\u2014why is Kubelet there? Wasn't it supposed to run only on worker nodes?</p> <p>You double-check your understanding: worker nodes handle workloads, and Kubelet is responsible for managing pods on those nodes. Control plane nodes, on the other hand, orchestrate everything but don\u2019t run regular workloads. So, why does <code>ps aux | grep kubelet</code> show Kubelet actively running on a control plane node?</p> <p>You start to wonder\u2014if control plane nodes aren\u2019t supposed to run application pods, does Kubelet even serve a purpose here?</p>"},{"location":"architecture_myths/myth1/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Worker nodes are responsible for running workloads, leading many to believe Kubelet is exclusive to them.</li> <li>Control plane nodes don\u2019t schedule regular workloads, making Kubelet\u2019s presence less noticeable.</li> <li>Many tutorials and diagrams oversimplify by associating Kubelet only with worker nodes.</li> </ol>"},{"location":"architecture_myths/myth1/#the-reality","title":"The Reality","text":"<p>Kubelet runs on all nodes in a typical Kubernetes cluster, including control plane nodes. However, its role differs: - On worker nodes: Kubelet registers the node and manages pod execution. - On control plane nodes (when using static pods): Kubelet ensures control plane components like the API server, scheduler, and controller manager run correctly.</p> <p>Exception: Some Kubernetes distributions (e.g., certain air-gapped or enterprise setups) run control plane components as systemd services instead of static pods. In such cases, Kubelet may not be needed on control plane nodes. However, this is uncommon in most Kubernetes distributions.</p>"},{"location":"architecture_myths/myth1/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check Kubelet Running on Control Plane Nodes</p> <pre><code>ssh &lt;control-plane-node&gt;\nps aux | grep kubelet\n</code></pre> <p>If Kubelet is running, you\u2019ll see its process listed.</p> <p>Step 2: Verify the Kubelet service status (For Most Kubernetes Setups)</p> <pre><code>ssh &lt;control-plane-node&gt;\nsystemctl status kubelet --no-pager\n</code></pre> <p>If active, Kubelet is running on the control plane node.</p> <p>Step 3: Confirm Kubelet's Role in Managing Static Pods Since Kubelet manages static pods on control plane nodes, you can check for static pod manifests:</p> <pre><code>ls /etc/kubernetes/manifests/\n</code></pre> <p>If you see files like kube-apiserver.yaml and etcd.yaml, Kubelet is responsible for running control plane components as static pods.</p>"},{"location":"architecture_myths/myth1/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubelet runs on all worker nodes and is commonly present on control plane nodes.</li> <li>On worker nodes, Kubelet registers the node and manages workload execution.</li> <li>On control plane nodes, Kubelet typically manages static pods for core components unless systemd-based services are used.</li> <li>Understanding Kubelet\u2019s role is essential for troubleshooting and maintaining cluster stability.</li> </ul>"},{"location":"architecture_myths/myth2/","title":"Myth 2: Kubernetes Clusters Can't Function Without Kube-Proxy","text":"<p>You deploy a Kubernetes cluster and start checking the usual system components. API server? Running. Controller manager? Running. Kube-Proxy? Wait\u2026 it's missing! You double-check the namespace, logs, and even the deployment\u2014nothing. But surprisingly, your pods and services are still communicating just fine. How is this possible? Isn't Kube-Proxy essential for cluster networking?</p>"},{"location":"architecture_myths/myth2/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Kube-Proxy is enabled by default in most Kubernetes distributions, making it appear mandatory.</li> <li>Traditional Kubernetes networking relies on iptables or IPVS, which are managed by Kube-Proxy, reinforcing its perceived necessity.</li> <li>Many engineers are unaware of eBPF-based networking solutions, such as Cilium, which can entirely replace Kube-Proxy while improving performance and scalability.</li> </ol>"},{"location":"architecture_myths/myth2/#reality","title":"Reality","text":"<ul> <li>Kube-Proxy is a default networking component in Kubernetes, but it is not mandatory for cluster functionality. Kubernetes networking is designed to support multiple implementations, and modern solutions can replace Kube-Proxy entirely.</li> <li>Newer networking models, such as eBPF-based CNIs (e.g., Cilium, Calico eBPF, and Katran), bypass the need for Kube-Proxy by directly handling packet processing within the kernel. These solutions not only eliminate the dependency on iptables/IPVS but also improve performance, scalability, and security by reducing latency and enabling fine-grained traffic control.</li> <li>Many high-performance Kubernetes clusters\u2014especially those optimized for large-scale workloads\u2014choose to disable Kube-Proxy in favor of these more efficient networking alternatives.</li> </ul>"},{"location":"architecture_myths/myth2/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Disable Kube-Proxy Scale down Kube-Proxy to remove it from the cluster:</p> <pre><code>kubectl -n kube-system scale deployment kube-proxy --replicas=0\n</code></pre> <p>Step 2: Install Cilium Without Kube-Proxy Use Cilium as a replacement, enabling eBPF-based networking:</p> <pre><code>helm install cilium cilium/cilium --set kubeProxyReplacement=strict\n</code></pre> <p>Step 3: Deploy a Service and Test Connectivity Create an Nginx pod and expose it as a service:</p> <pre><code>kubectl run nginx --image=nginx --port=80 --expose\n</code></pre> <p>Now, check if the service works without Kube-Proxy:</p> <pre><code>kubectl run test-pod --image=busybox --restart=Never --rm -it -- wget -qO- nginx\n</code></pre> <p>If successful, you\u2019ll see the Nginx welcome page, proving that Kubernetes networking works without Kube-Proxy.</p>"},{"location":"architecture_myths/myth2/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kube-Proxy is NOT mandatory\u2014Kubernetes networking can function without it.</li> <li>eBPF-based CNIs (Cilium, Calico eBPF, etc.) can fully replace Kube-Proxy, offering better performance and scalability.</li> <li>Choosing the right networking approach is crucial\u2014understanding Kubernetes networking beyond Kube-Proxy helps optimize cluster efficiency, security, and observability.</li> </ul>"},{"location":"architecture_myths/myth3/","title":"Myth 3: Kubernetes Networking Works Fine Without a CNI Plugin","text":"<p>You set up a Production Kubernetes cluster, deploy some pods, and\u2026 nothing. They can't talk to each other, You search Stack Overflow, try restarting pods, but nothing works, Finally, you realize networking is broken, and some pods are stuck in \"ContainerCreating\" state. You check the logs and see:</p> <pre><code>NetworkPluginNotReady: CNI plugin not initialized\n</code></pre> <p>So, What might be wrong? </p>"},{"location":"architecture_myths/myth3/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Local environments create an illusion \u2013 Some Kubernetes distributions (like Minikube, k3s, and kind) include built-in networking, making it seem like a CNI is optional. Since these setups \"just work\" out of the box, users may not realize a CNI is missing in a standard cluster.</li> <li>Kubernetes does not bundle a default CNI \u2013 Unlike other components (like the scheduler or API server), Kubernetes does not ship with a preinstalled CNI. This can mislead users into thinking networking is an add-on rather than a fundamental requirement.</li> <li>Single-node clusters work without a CNI \u2013 On a single-node cluster, all pods can share the host\u2019s networking stack. This makes basic pod communication seem functional, masking the fact that a proper CNI is required for multi-node networking, pod IP assignment, and policies.</li> </ol>"},{"location":"architecture_myths/myth3/#the-reality","title":"The Reality:","text":"<p>A CNI (Container Network Interface) plugin is essential for Production Grade Kubernetes networking\u2014without it, core networking functionalities break. - Pod-to-pod communication depends on CNI \u2013 Kubernetes does not manage networking by itself; it relies on a CNI to assign IP addresses and enable connectivity between pods, especially across nodes. - Multi-node clusters require a CNI \u2013 Without a CNI, pods on different nodes cannot communicate because Kubernetes does not provide an internal networking layer. - Critical Kubernetes features won\u2019t work \u2013 Services, network policies, and pod IP management all rely on a CNI. Without it, pods may remain stuck in the <code>ContainerCreating</code> state, and basic networking between workloads will fail.</p> <p>Even though local setups like k3s and Minikube may seem to work without a visible CNI,this is because they embed lightweight networking solutions. In a standard Kubernetes cluster, a CNI is mandatory for networking to function.</p>"},{"location":"architecture_myths/myth3/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check Pod Status Without a CNI If a Kubernetes cluster is deployed without a CNI, pods will be stuck in the ContainerCreating state. Run:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Example output:</p> <pre><code>NAMESPACE     NAME                          READY   STATUS              RESTARTS   AGE\nkube-system   coredns-78fcd69978-2v7        0/1     ContainerCreating   0          10m\nkube-system   coredns-78fcd69978-s7jxp      0/1     ContainerCreating   0          10m\n\n</code></pre> <p>Step 2: Check Node Conditions Inspect the node description to verify networking errors:</p> <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre> <p>You'll likely see an error like:</p> <pre><code>NetworkPluginNotReady: network plugin is not ready: cni plugin not initialized\n</code></pre> <p>Step 3: Verify Missing Network Interfaces Kubernetes assigns pod IPs through the CNI. If no CNI is installed, pods won\u2019t receive IPs:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>If the IP column is empty or missing, it confirms that pod networking is broken due to a missing CNI.</p>"},{"location":"architecture_myths/myth3/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes delegates networking to a CNI, making it a required component.</li> <li>Without a CNI, pods won\u2019t get IPs, and multi-node clusters won\u2019t work.</li> <li>Some local environments provide built-in networking, making CNIs seem optional..</li> <li>Always verify a CNI is installed to ensure Kubernetes networking works as expected.</li> </ul>"},{"location":"architecture_myths/myth4/","title":"Myth 4: Control Plane Nodes Don\u2019t Need a Container Runtime","text":"<p>You set up your control plane node, confident that everything is configured correctly. Next, you run <code>kubeadm init</code>, expecting a smooth setup\u2014but it fails instantly!</p> <p>The error? <code>no container runtime detected.</code></p> <p>That\u2019s strange. Isn\u2019t the container runtime needed only on worker nodes to run application workloads? You double-check your configuration, thinking you might have missed a step. But the error persists.</p> <p>What\u2019s going on here? Why is Kubernetes complaining about a missing CRI on the control plane?</p>"},{"location":"architecture_myths/myth4/#why-does-this-myth-exist","title":"Why Does This Myth Exist?","text":"<ol> <li>Over-Simplified Architecture Explanations \u2013 Many Kubernetes learning resources describe the architecture in a rigid way: Control plane runs API Server, etcd, Scheduler, and Controller Manager, while worker nodes run Kubelet, CRI, and Kube-Proxy. This oversimplification leads to misconceptions.</li> <li>Misinterpretation of Control Plane Responsibilities \u2013 Since control plane components are often seen as \"management-only,\" people assume they don\u2019t rely on container runtimes, networking components, or Kubelet.</li> <li>Managed Kubernetes Abstraction \u2013 Many managed Kubernetes services abstract away the control plane, making engineers believe that only worker nodes require components like CRI, Kubelet, or CNI.</li> <li>Historical Understanding of Kubernetes \u2013 Earlier Kubernetes documentation and tutorials emphasized worker nodes as the place where \"real workloads\" run, reinforcing the belief that key infrastructure components don\u2019t function similarly on control plane nodes.</li> </ol>"},{"location":"architecture_myths/myth4/#the-reality","title":"The Reality","text":"<p>CRI is Essential on Control Plane Nodes\u2014But with Exceptions: - When control plane components (like API Server, Controller Manager, and etcd) run as static pods, a Container Runtime Interface (CRI) is required because Kubelet manages them as containers. - However, in some Kubernetes setups (e.g., certain managed services or custom-built clusters), control plane components run as systemd services instead of static pods. In such cases, the control plane can function without a CRI. - Despite this exception, most Kubernetes distributions rely on static pods, making CRI a fundamental requirement for both control plane and worker nodes. - Even if the control plane runs without a CRI, worker nodes still require it to manage application workloads.</p>"},{"location":"architecture_myths/myth4/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check if the Control Plane Uses CRI Run:</p> <pre><code>kubectl get pods -n kube-system\n</code></pre> <p>Expected Output (If CRI is Present):</p> <pre><code>NAME                               READY   STATUS    RESTARTS   AGE\nkube-apiserver-control-plane       1/1     Running   0          5m\netcd-control-plane                 1/1     Running   0          5m\n</code></pre> <p>If these pods are missing or stuck in <code>ContainerCreating</code>, it likely means CRI is missing.</p> <p>Step 2: Verify CRI Connectivity Run:</p> <pre><code>crictl ps\n</code></pre> <p>Expected Output (If CRI is Missing):</p> <pre><code>E0205 10:22:34.123456   1234 runtime.go:300] no runtime configured\n</code></pre> <p>Confirming that the control plane needs a CRI just like worker nodes.</p>"},{"location":"architecture_myths/myth4/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>CRI is required on all nodes where Kubernetes components run as containers, including control plane nodes.</li> <li>Control plane components (like API Server, etcd, and Controller Manager) run as static pods in most Kubernetes setups, requiring a container runtime.</li> <li>Some Kubernetes distributions run control plane components as systemd services, in which case CRI is not needed on control plane nodes.</li> <li>Without a CRI, kubeadm cannot initialize the cluster, and critical control plane services won\u2019t start if they rely on containers.</li> <li>Understanding CRI\u2019s role helps in troubleshooting missing services and failed cluster initialization issues.</li> </ul>"},{"location":"general_myths/overview/","title":"Work in progress","text":""},{"location":"networking_myths/overview/","title":"Work in progress","text":""},{"location":"pod_myths/overview/","title":"Work in progress","text":""},{"location":"workload_myths/overview/","title":"Work in progress","text":""}]}