{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Kubernetes Myths","text":"<p>Kubernetes Myths is a deep dive into the most common misconceptions surrounding Kubernetes. Kubernetes Myths is a living, developer-focused resource that clears up common misconceptions about how Kubernetes really works.</p> <p>Born out of real-world experience, production incidents, and deep dives into Kubernetes source code, this site aims to bridge the gap between what people assume and how things actually work.</p> <p>Whether you're a beginner trying to grasp Kubernetes fundamentals or a seasoned engineer looking to refine your expertise, this book challenges what you \"think\" you know. Each myth is dissected with a backstory, technical analysis, and a step-by-step validation\u2014so you don\u2019t just read, you prove it yourself.</p>"},{"location":"#who-should-read-this-book","title":"Who Should Read This Book?","text":"<p>This book is for:</p> <ul> <li> <p>Platform Engineers &amp; DevOps Practitioners building scalable Kubernetes infrastructure.</p> </li> <li> <p>Developers who want to understand Kubernetes beyond just kubectl apply.</p> </li> <li> <p>SREs &amp; Architects designing secure and resilient cloud-native systems.</p> </li> <li> <p>Anyone who has hit unexpected Kubernetes behavior and wondered why.</p> </li> </ul> <p>Even if you consider yourself an expert, this book will challenge some of your assumptions. Some of these myths might surprise you.</p>"},{"location":"#how-this-book-is-structured","title":"How This Book is Structured","text":"<p>Each chapter follows a structured approach:</p> <ul> <li> <p>Myth \u2013 The common misconception.</p> </li> <li> <p>Why This Myth Exists \u2013 How and why misinformation spreads.</p> </li> <li> <p>Reality \u2013 What actually happens in Kubernetes.</p> </li> <li> <p>Experiment &amp; Validate \u2013  Real-world evidence to show reality</p> </li> <li> <p>Key Takeaways \u2013 A quick recap of the core lessons.</p> </li> </ul>"},{"location":"about/","title":"About","text":"<p>Hi, I'm Rajesh Deshpande \u2014 a cloud-native engineer passionate about Kubernetes, Golang, and platform engineering. With over 13 years of experience in software engineering, I created KubernetesMyths.com to bust misconceptions and share deep, code-level insights.</p>"},{"location":"architecture_myths/Myth1_Kubelet_is_Exclusive_to_Worker_Nodes/","title":"Myth 1: Kubelet is Exclusive to Worker Nodes","text":"<p>You SSH into a control plane node, expecting to see only control plane components like the API server, controller manager, and scheduler running. But wait\u2014why is Kubelet there? Wasn't it supposed to run only on worker nodes?</p> <p>You double-check your understanding: worker nodes handle workloads, and Kubelet is responsible for managing pods on those nodes. Control plane nodes, on the other hand, orchestrate everything but don\u2019t run regular workloads. So, why does <code>ps aux | grep kubelet</code> show Kubelet actively running on a control plane node?</p> <p>You start to wonder\u2014if control plane nodes aren\u2019t supposed to run application pods, does Kubelet even serve a purpose here?</p>"},{"location":"architecture_myths/Myth1_Kubelet_is_Exclusive_to_Worker_Nodes/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Worker nodes are responsible for running workloads, leading many to believe Kubelet is exclusive to them.</li> <li>Control plane nodes don\u2019t schedule regular workloads, making Kubelet\u2019s presence less noticeable.</li> <li>Many tutorials and diagrams oversimplify by associating Kubelet only with worker nodes.</li> </ol>"},{"location":"architecture_myths/Myth1_Kubelet_is_Exclusive_to_Worker_Nodes/#the-reality","title":"The Reality","text":"<p>Kubelet runs on all nodes in a typical Kubernetes cluster, including control plane nodes. However, its role differs:</p> <ul> <li> <p>On worker nodes: Kubelet registers the node and manages pod execution.</p> </li> <li> <p>On control plane nodes (when using static pods): Kubelet ensures control plane components like the API server, scheduler, and controller manager run correctly.</p> </li> </ul> <p>Exception: Some Kubernetes distributions (e.g., certain air-gapped or enterprise setups) run control plane components as systemd services instead of static pods. In such cases, Kubelet may not be needed on control plane nodes. However, this is uncommon in most Kubernetes distributions.</p>"},{"location":"architecture_myths/Myth1_Kubelet_is_Exclusive_to_Worker_Nodes/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check Kubelet Running on Control Plane Nodes</p> <pre><code>ssh &lt;control-plane-node&gt;\nps aux | grep kubelet\n</code></pre> <p>If Kubelet is running, you\u2019ll see its process listed.</p> <p>Step 2: Verify the Kubelet service status (For Most Kubernetes Setups)</p> <pre><code>ssh &lt;control-plane-node&gt;\nsystemctl status kubelet --no-pager\n</code></pre> <p>If active, Kubelet is running on the control plane node.</p> <p>Step 3: Confirm Kubelet's Role in Managing Static Pods Since Kubelet manages static pods on control plane nodes, you can check for static pod manifests:</p> <pre><code>ls /etc/kubernetes/manifests/\n</code></pre> <p>If you see files like kube-apiserver.yaml and etcd.yaml, Kubelet is responsible for running control plane components as static pods.</p>"},{"location":"architecture_myths/Myth1_Kubelet_is_Exclusive_to_Worker_Nodes/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubelet runs on all worker nodes and is commonly present on control plane nodes.</li> <li>On worker nodes, Kubelet registers the node and manages workload execution.</li> <li>On control plane nodes, Kubelet typically manages static pods for core components unless systemd-based services are used.</li> <li>Understanding Kubelet\u2019s role is essential for troubleshooting and maintaining cluster stability.</li> </ul>"},{"location":"architecture_myths/Myth2_Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/","title":"Myth 2: Kubernetes Clusters Can't Function Without Kube-Proxy","text":"<p>You deploy a Kubernetes cluster and start checking the usual system components. API server? Running. Controller manager? Running. Kube-Proxy? Wait\u2026 it's missing! You double-check the namespace, logs, and even the deployment\u2014nothing. But surprisingly, your pods and services are still communicating just fine. How is this possible? Isn't Kube-Proxy essential for cluster networking?</p>"},{"location":"architecture_myths/Myth2_Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Kube-Proxy is enabled by default in most Kubernetes distributions, making it appear mandatory.</li> <li>Traditional Kubernetes networking relies on iptables or IPVS, which are managed by Kube-Proxy, reinforcing its perceived necessity.</li> <li>Many engineers are unaware of eBPF-based networking solutions, such as Cilium, which can entirely replace Kube-Proxy while improving performance and scalability.</li> </ol>"},{"location":"architecture_myths/Myth2_Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/#reality","title":"Reality","text":"<ul> <li>Kube-Proxy is a default networking component in Kubernetes, but it is not mandatory for cluster functionality. Kubernetes networking is designed to support multiple implementations, and modern solutions can replace Kube-Proxy entirely.</li> <li>Newer networking models, such as eBPF-based CNIs (e.g., Cilium, Calico eBPF, and Katran), bypass the need for Kube-Proxy by directly handling packet processing within the kernel. These solutions not only eliminate the dependency on iptables/IPVS but also improve performance, scalability, and security by reducing latency and enabling fine-grained traffic control.</li> <li>Many high-performance Kubernetes clusters\u2014especially those optimized for large-scale workloads\u2014choose to disable Kube-Proxy in favor of these more efficient networking alternatives.</li> </ul>"},{"location":"architecture_myths/Myth2_Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Disable Kube-Proxy Scale down Kube-Proxy to remove it from the cluster:</p> <pre><code>kubectl -n kube-system scale deployment kube-proxy --replicas=0\n</code></pre> <p>Step 2: Install Cilium Without Kube-Proxy Use Cilium as a replacement, enabling eBPF-based networking:</p> <pre><code>helm install cilium cilium/cilium --set kubeProxyReplacement=strict\n</code></pre> <p>Step 3: Deploy a Service and Test Connectivity Create an Nginx pod and expose it as a service:</p> <pre><code>kubectl run nginx --image=nginx --port=80 --expose\n</code></pre> <p>Now, check if the service works without Kube-Proxy:</p> <pre><code>kubectl run test-pod --image=busybox --restart=Never --rm -it -- wget -qO- nginx\n</code></pre> <p>If successful, you\u2019ll see the Nginx welcome page, proving that Kubernetes networking works without Kube-Proxy.</p>"},{"location":"architecture_myths/Myth2_Kubernetes_Clusters_Can%27t_Function_Without_Kube-Proxy/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kube-Proxy is NOT mandatory\u2014Kubernetes networking can function without it.</li> <li>eBPF-based CNIs (Cilium, Calico eBPF, etc.) can fully replace Kube-Proxy, offering better performance and scalability.</li> <li>Choosing the right networking approach is crucial\u2014understanding Kubernetes networking beyond Kube-Proxy helps optimize cluster efficiency, security, and observability.</li> </ul>"},{"location":"architecture_myths/Myth3_Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/","title":"Myth 3: Kubernetes Networking Works Fine Without a CNI Plugin","text":"<p>You set up a Production Kubernetes cluster, deploy some pods, and\u2026 nothing. They can't talk to each other, You search Stack Overflow, try restarting pods, but nothing works, Finally, you realize networking is broken, and some pods are stuck in \"ContainerCreating\" state. You check the logs and see:</p> <pre><code>NetworkPluginNotReady: CNI plugin not initialized\n</code></pre> <p>So, What might be wrong? </p>"},{"location":"architecture_myths/Myth3_Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Local environments create an illusion \u2013 Some Kubernetes distributions (like Minikube, k3s, and kind) include built-in networking, making it seem like a CNI is optional. Since these setups \"just work\" out of the box, users may not realize a CNI is missing in a standard cluster.</li> <li>Kubernetes does not bundle a default CNI \u2013 Unlike other components (like the scheduler or API server), Kubernetes does not ship with a preinstalled CNI. This can mislead users into thinking networking is an add-on rather than a fundamental requirement.</li> <li>Single-node clusters work without a CNI \u2013 On a single-node cluster, all pods can share the host\u2019s networking stack. This makes basic pod communication seem functional, masking the fact that a proper CNI is required for multi-node networking, pod IP assignment, and policies.</li> </ol>"},{"location":"architecture_myths/Myth3_Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/#the-reality","title":"The Reality:","text":"<p>A CNI (Container Network Interface) plugin is essential for Production Grade Kubernetes networking\u2014without it, core networking functionalities break.</p> <ul> <li> <p>Pod-to-pod communication depends on CNI \u2013 Kubernetes does not manage networking by itself; it relies on a CNI to assign IP addresses and enable connectivity between pods, especially across nodes.</p> </li> <li> <p>Multi-node clusters require a CNI \u2013 Without a CNI, pods on different nodes cannot communicate because Kubernetes does not provide an internal networking layer.</p> </li> <li> <p>Critical Kubernetes features won\u2019t work \u2013 Services, network policies, and pod IP management all rely on a CNI. Without it, pods may remain stuck in the <code>ContainerCreating</code> state, and basic networking between workloads will fail.</p> </li> </ul> <p>Even though local setups like k3s and Minikube may seem to work without a visible CNI,this is because they embed lightweight networking solutions. In a standard Kubernetes cluster, a CNI is mandatory for networking to function.</p>"},{"location":"architecture_myths/Myth3_Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check Pod Status Without a CNI If a Kubernetes cluster is deployed without a CNI, pods will be stuck in the ContainerCreating state. Run:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Example output:</p> <pre><code>NAMESPACE     NAME                          READY   STATUS              RESTARTS   AGE\nkube-system   coredns-78fcd69978-2v7        0/1     ContainerCreating   0          10m\nkube-system   coredns-78fcd69978-s7jxp      0/1     ContainerCreating   0          10m\n\n</code></pre> <p>Step 2: Check Node Conditions Inspect the node description to verify networking errors:</p> <pre><code>kubectl describe node &lt;node-name&gt;\n</code></pre> <p>You'll likely see an error like:</p> <pre><code>NetworkPluginNotReady: network plugin is not ready: cni plugin not initialized\n</code></pre> <p>Step 3: Verify Missing Network Interfaces Kubernetes assigns pod IPs through the CNI. If no CNI is installed, pods won\u2019t receive IPs:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>If the IP column is empty or missing, it confirms that pod networking is broken due to a missing CNI.</p>"},{"location":"architecture_myths/Myth3_Kubernetes_Networking_Works_Fine_Without_a_CNI_Plugin/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes delegates networking to a CNI, making it a required component.</li> <li>Without a CNI, pods won\u2019t get IPs, and multi-node clusters won\u2019t work.</li> <li>Some local environments provide built-in networking, making CNIs seem optional..</li> <li>Always verify a CNI is installed to ensure Kubernetes networking works as expected.</li> </ul>"},{"location":"architecture_myths/Myth4_Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/","title":"Myth 4: Control Plane Nodes Don\u2019t Need a Container Runtime","text":"<p>You set up your control plane node, confident that everything is configured correctly. Next, you run <code>kubeadm init</code>, expecting a smooth setup\u2014but it fails instantly!</p> <p>The error? <code>no container runtime detected.</code></p> <p>That\u2019s strange. Isn\u2019t the container runtime needed only on worker nodes to run application workloads? You double-check your configuration, thinking you might have missed a step. But the error persists.</p> <p>What\u2019s going on here? Why is Kubernetes complaining about a missing CRI on the control plane?</p>"},{"location":"architecture_myths/Myth4_Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/#why-does-this-myth-exist","title":"Why Does This Myth Exist?","text":"<ol> <li>Over-Simplified Architecture Explanations \u2013 Many Kubernetes learning resources describe the architecture in a rigid way: Control plane runs API Server, etcd, Scheduler, and Controller Manager, while worker nodes run Kubelet, CRI, and Kube-Proxy. This oversimplification leads to misconceptions.</li> <li>Misinterpretation of Control Plane Responsibilities \u2013 Since control plane components are often seen as \"management-only,\" people assume they don\u2019t rely on container runtimes, networking components, or Kubelet.</li> <li>Managed Kubernetes Abstraction \u2013 Many managed Kubernetes services abstract away the control plane, making engineers believe that only worker nodes require components like CRI, Kubelet, or CNI.</li> <li>Historical Understanding of Kubernetes \u2013 Earlier Kubernetes documentation and tutorials emphasized worker nodes as the place where \"real workloads\" run, reinforcing the belief that key infrastructure components don\u2019t function similarly on control plane nodes.</li> </ol>"},{"location":"architecture_myths/Myth4_Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/#the-reality","title":"The Reality","text":"<p>CRI is Essential on Control Plane Nodes\u2014But with Exceptions:</p> <ul> <li> <p>When control plane components (like API Server, Controller Manager, and etcd) run as static pods, a Container Runtime Interface (CRI) is required because Kubelet manages them as containers.</p> </li> <li> <p>However, in some Kubernetes setups (e.g., certain managed services or custom-built clusters), control plane components run as systemd services instead of static pods. In such cases, the control plane can function without a CRI.</p> </li> <li> <p>Despite this exception, most Kubernetes distributions rely on static pods, making CRI a fundamental requirement for both control plane and worker nodes.</p> </li> <li> <p>Even if the control plane runs without a CRI, worker nodes still require it to manage application workloads.</p> </li> </ul>"},{"location":"architecture_myths/Myth4_Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Check if the Control Plane Uses CRI Run:</p> <pre><code>kubectl get pods -n kube-system\n</code></pre> <p>Expected Output (If CRI is Present):</p> <pre><code>NAME                               READY   STATUS    RESTARTS   AGE\nkube-apiserver-control-plane       1/1     Running   0          5m\netcd-control-plane                 1/1     Running   0          5m\n</code></pre> <p>If these pods are missing or stuck in <code>ContainerCreating</code>, it likely means CRI is missing.</p> <p>Step 2: Verify CRI Connectivity Run:</p> <pre><code>crictl ps\n</code></pre> <p>Expected Output (If CRI is Missing):</p> <pre><code>E0205 10:22:34.123456   1234 runtime.go:300] no runtime configured\n</code></pre> <p>Confirming that the control plane needs a CRI just like worker nodes.</p>"},{"location":"architecture_myths/Myth4_Control_Plane_Nodes_Don%E2%80%99t_Need_a_Container_Runtime/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>CRI is required on all nodes where Kubernetes components run as containers, including control plane nodes.</li> <li>Control plane components (like API Server, etcd, and Controller Manager) run as static pods in most Kubernetes setups, requiring a container runtime.</li> <li>Some Kubernetes distributions run control plane components as systemd services, in which case CRI is not needed on control plane nodes.</li> <li>Without a CRI, kubeadm cannot initialize the cluster, and critical control plane services won\u2019t start if they rely on containers.</li> <li>Understanding CRI\u2019s role helps in troubleshooting missing services and failed cluster initialization issues.</li> </ul>"},{"location":"architecture_myths/overview/","title":"Kubernetes Architecture Myths Overview","text":"<p>Kubernetes architecture is often perceived as a black box\u2014with terms like \"control plane,\" \"kubelet,\" and \"scheduler\" thrown around without deep clarity. This section tackles myths rooted in these components, where surface-level understanding leads to poor design choices and debugging frustrations.</p> <p>From misunderstanding how the scheduler works to misattributing responsibilities between components like the kube-apiserver and kubelet, these myths expose critical gaps in architectural knowledge.</p> <p>By busting these myths, we aim to deepen your mental model of Kubernetes and help you design more reliable, secure, and efficient clusters.</p>"},{"location":"architecture_myths/overview/#subtopics","title":"Subtopics","text":"<ul> <li>Myth 1: Kubelet is Exclusive to Worker Nodes </li> <li>Myth 2: Kubernetes Clusters Can't Function Without Kube-Proxy </li> <li>Myth 3: Kubernetes Networking Works Fine Without a CNI Plugin</li> <li>Myth 4: Control Plane Nodes Don\u2019t Need a Container Runtime</li> </ul>"},{"location":"general_myths/Myth1_Kubernetes_Namespaces_Provide_Complete_Isolation/","title":"Myth 1: Kubernetes Namespaces Provide Complete Isolation","text":"<p>Many teams assume that creating separate namespaces guarantees strong isolation between workloads, preventing them from affecting each other. But this belief can lead to critical security oversights.</p> <p>A developer once confidently deployed production and staging workloads in separate namespaces on the same cluster, believing they were fully isolated. Later, a misconfigured role binding allowed an engineer to access production resources from the staging namespace\u2014resulting in an unintended outage.</p>"},{"location":"general_myths/Myth1_Kubernetes_Namespaces_Provide_Complete_Isolation/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<p>This myth persists because namespaces feel like isolated environments in practice. They offer separate dashboards, configs, and resource groupings \u2014 which gives the illusion of sandboxing. Here\u2019s why people are misled:</p> <ul> <li>Visual &amp; Logical Separation:</li> </ul> <p>Tools like kubectl, dashboards, and YAML files naturally group resources by namespace, leading engineers to assume there's also runtime separation.</p> <ul> <li>RBAC Defaults Are Namespace-Scoped:</li> </ul> <p>Since many RBAC policies are written per-namespace, teams assume the underlying access and execution contexts are automatically isolated.</p> <ul> <li>Cloud Vendors Encourage Namespaces for Multi-Tenancy:</li> </ul> <p>Many best practices and tutorials recommend using namespaces for separating teams or environments \u2014 without emphasizing the limitations of that approach.</p> <ul> <li>No Immediate Breakage in Small Projects:</li> </ul> <p>In development or staging setups, cross-namespace issues are rare \u2014 so teams wrongly assume this behavior holds in production environments too.</p> <ul> <li>Terminology Confusion:</li> </ul> <p>The term \u201cnamespace\u201d is borrowed from programming (e.g., Java/C++), where it usually does imply hard boundaries. This contributes to the false sense of isolation.</p>"},{"location":"general_myths/Myth1_Kubernetes_Namespaces_Provide_Complete_Isolation/#the-reality","title":"The Reality:","text":"<p>Namespaces are a convenience feature for organizing workloads, not a security mechanism. Here\u2019s what they actually provide:</p> <ul> <li> <p>Logical separation of Kubernetes objects (pods, services, secrets, etc.).</p> </li> <li> <p>Quota enforcement to limit resource consumption per namespace.</p> </li> <li> <p>RBAC (Role-Based Access Control) to define access policies, but not by default.</p> </li> </ul> <p>But they do not:</p> <ul> <li> <p>Prevent cross-namespace network traffic.</p> </li> <li> <p>Guarantee CPU/memory isolation across namespaces.</p> </li> <li> <p>Restrict access without proper RBAC and Network Policies.</p> </li> </ul>"},{"location":"general_myths/Myth1_Kubernetes_Namespaces_Provide_Complete_Isolation/#experiment-validate","title":"Experiment &amp; Validate","text":"<ol> <li>Create two namespaces:</li> </ol> <pre><code>kubectl create namespace dev  \nkubectl create namespace prod  \n</code></pre> <ol> <li>Deploy a test pod in each:</li> </ol> <pre><code>kubectl run nginx-dev --image=nginx -n dev  \nkubectl run nginx-prod --image=nginx -n prod  \n</code></pre> <ol> <li>From the dev namespace, try resolving a service in prod:</li> </ol> <pre><code>kubectl exec -n dev nginx-dev -- nslookup nginx-prod.prod.svc.cluster.local \n</code></pre> <p>Surprise! The pod in dev can still resolve services in prod, proving that namespaces don\u2019t block network communication by default.</p>"},{"location":"general_myths/Myth1_Kubernetes_Namespaces_Provide_Complete_Isolation/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Namespaces help organize resources but do not enforce strong isolation.</li> <li>Security and access control must be explicitly defined using RBAC and Network Policies.</li> <li>For strict separation, consider multi-cluster architectures.</li> </ul>"},{"location":"general_myths/Myth2_Complete_application_can_be_rolled_back_in_Kubernetes/","title":"Myth 2: Complete application can be rolled back in Kubernetes","text":"<p>A team deployed a new version of their microservices-based application in Kubernetes. Soon after, they noticed critical issues and decided to roll back using:</p> <pre><code>kubectl rollout undo deployment my-app\n</code></pre> <p>They expected everything to return to normal instantly. Instead, they faced database inconsistencies, mismatched API versions, and persistent storage corruption. Their rollback didn\u2019t restore the full application state\u2014only the container images reverted.</p>"},{"location":"general_myths/Myth2_Complete_application_can_be_rolled_back_in_Kubernetes/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<p>This myth persists because many assume Kubernetes rollbacks work like traditional application restore mechanisms. The key reasons behind this misconception are:</p> <ol> <li> <p>Rollback Command Appears Comprehensive \u2013 The <code>kubectl rollout undo</code> command suggests it restores everything, but it only affects Deployments, not databases, ConfigMaps, or storage.</p> </li> <li> <p>Stateless vs. Stateful Confusion \u2013 Stateless apps can be rolled back easily, but stateful workloads (databases, message queues, persistent volumes) require additional rollback strategies.</p> </li> <li> <p>Lack of Awareness About External Dependencies \u2013 Applications in Kubernetes often depend on external databases, cloud storage, or third-party services, which are unaffected by a simple rollback.</p> </li> <li> <p>Expectation from Traditional Monolithic Rollbacks \u2013 In legacy systems, rolling back a single instance often meant restoring the full application state, whereas Kubernetes primarily focuses on container lifecycle management.</p> </li> <li> <p>Kubernetes Default Behavior is Not Full Recovery \u2013 Kubernetes prioritizes high availability and rolling updates but does not track or revert external dependencies automatically.</p> </li> </ol> <p>This misunderstanding leads teams to believe that a simple Kubernetes rollback will restore the entire application state when, in reality, it only reverts container versions and pod specifications.</p>"},{"location":"general_myths/Myth2_Complete_application_can_be_rolled_back_in_Kubernetes/#the-reality","title":"The Reality:","text":"<p>Rolling back a deployment does not mean your entire application is restored. Kubernetes can revert container images and pod specifications, but:</p> <ul> <li> <p>Databases Require Explicit Rollbacks \u2013 Schema migrations must be reversible or manually handled.</p> </li> <li> <p>Persistent Volumes Retain Data \u2013 Rolling back an app doesn\u2019t roll back stored data unless snapshots or backups exist.</p> </li> <li> <p>Configuration Drift Can Occur \u2013 Changes in ConfigMaps or Secrets won\u2019t automatically revert unless managed separately.</p> </li> <li> <p>Interdependent Services Can Break \u2013 A rollback might mismatch versions between microservices, causing API incompatibilities.</p> </li> </ul>"},{"location":"general_myths/Myth2_Complete_application_can_be_rolled_back_in_Kubernetes/#experiment-validate","title":"Experiment &amp; Validate","text":"<ol> <li>Deploy an app and apply a schema migration</li> </ol> <pre><code>kubectl create deployment my-app --image=my-app:v1  \nkubectl apply -f database-migration.yaml  # Applies a schema change  \n</code></pre> <ol> <li>Roll back the deployment</li> </ol> <pre><code>kubectl rollout undo deployment my-app  \n</code></pre> <p>Result: The app\u2019s code is rolled back, but the database remains on the new schema, potentially breaking compatibility.</p>"},{"location":"general_myths/Myth2_Complete_application_can_be_rolled_back_in_Kubernetes/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes rollbacks only revert Deployments, not the entire application state.</li> <li>Persistent data, ConfigMaps, and external services are unaffected by a rollback.</li> <li>Database schema changes, inter-service dependencies, and persistent storage require separate rollback strategies.</li> </ul>"},{"location":"general_myths/Myth3_Kubernetes_automatically_roll_back_failed_deployment/","title":"Myth 3: Kubernetes automatically roll back failed deployment","text":"<p>A team deployed a new version of their application, expecting Kubernetes to roll it back automatically if something went wrong. Unfortunately, the deployment failed, but instead of rolling back, Kubernetes left it in a bad state. The team was surprised\u2014wasn't Kubernetes supposed to handle this automatically?</p>"},{"location":"general_myths/Myth3_Kubernetes_automatically_roll_back_failed_deployment/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<p>This misunderstanding comes from:</p> <ol> <li> <p>Expectation from Traditional Systems \u2013 Some deployment tools provide automatic rollback mechanisms, leading users to assume Kubernetes does the same.</p> </li> <li> <p>Confusion with Health Checks \u2013 Kubernetes can stop bad deployments using readiness and liveness probes, but stopping an update is different from rolling back.</p> </li> <li> <p>Rollback Feature Misinterpretation \u2013 Since <code>kubectl rollout undo</code> exists, people assume Kubernetes triggers it automatically.</p> </li> <li> <p>Expectation from CI/CD Pipelines \u2013 Some CI/CD tools like ArgoCD and Spinnaker offer auto-rollbacks, making users think Kubernetes itself provides this feature.</p> </li> <li> <p>Misunderstanding <code>progressDeadlineSeconds</code> \u2013 Many believe setting <code>progressDeadlineSeconds</code> triggers an automatic rollback, but it only marks the deployment as failed.</p> </li> </ol>"},{"location":"general_myths/Myth3_Kubernetes_automatically_roll_back_failed_deployment/#the-reality","title":"The Reality","text":"<p>Kubernetes does not automatically roll back a failed Deployment. While it detects failures, it requires manual intervention to revert to a previous version.Here\u2019s what actually happens:</p> <ul> <li> <p>If a Deployment update fails, Kubernetes pauses further updates but does not roll back.</p> </li> <li> <p>You must manually trigger a rollback using <code>kubectl rollout undo deployment &lt;name&gt;</code>.</p> </li> <li> <p>Kubernetes tracks revisions, but rollbacks only work for Deployments\u2014they do not revert ConfigMaps, Secrets, or Persistent Volumes.</p> </li> <li> <p><code>progressDeadlineSeconds</code> plays a crucial role \u2013 If a rollout does not complete within the configured <code>progressDeadlineSeconds</code>, the Deployment is marked as failed, but Kubernetes does not roll it back automatically. Instead, the rollout gets paused, requiring manual intervention.</p> </li> </ul>"},{"location":"general_myths/Myth3_Kubernetes_automatically_roll_back_failed_deployment/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a Working Deployment</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  progressDeadlineSeconds: 30\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:latest\n          ports:\n            - containerPort: 80\n</code></pre> <p>Apply the Deployment:</p> <pre><code>kubectl apply -f nginx-deployment.yaml\nkubectl rollout status deployment/nginx-deployment\n</code></pre> <p>This should show a successful rollout.</p> <p>Step 2: Trigger a Failed Deployment Now, let's break the deployment by using an invalid container image.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  progressDeadlineSeconds: 30\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx:doesnotexist  # Invalid image\n          ports:\n            - containerPort: 80\n</code></pre> <p>Apply the broken Deployment:</p> <pre><code>kubectl apply -f nginx-deployment.yaml\n</code></pre> <p>Check the rollout status:</p> <pre><code>kubectl rollout status deployment/nginx-deployment --timeout=60s\nkubectl rollout status deployment/nginx-deployment --timeout=60s\n</code></pre> <p>After <code>progressDeadlineSeconds</code> (30 seconds in this case), Kubernetes will mark the Deployment as failed, but it won\u2019t roll back automatically.</p> <p>Step 3: Verify That Kubernetes Does Not Roll Back Check the rollout history:</p> <pre><code>kubectl rollout history deployment/nginx-deployment\n</code></pre> <p>You'll see that Kubernetes has not reverted to the previous working version.</p> <p>Check the deployment status:</p> <pre><code>kubectl get deployment nginx-deployment -o jsonpath='{.status.conditions}'\n</code></pre> <p>The output will show a ProgressDeadlineExceeded condition but no rollback.</p>"},{"location":"general_myths/Myth3_Kubernetes_automatically_roll_back_failed_deployment/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes does not automatically roll back a failed Deployment.</li> <li>You must manually trigger a rollback if needed.</li> <li>progressDeadlineSeconds only marks a deployment as failed but does not roll it back.</li> <li>CI/CD tools can be configured to handle rollback logic.</li> </ul>"},{"location":"general_myths/overview/","title":"Kubernetes General Myths Overview","text":"<p>Kubernetes is powerful\u2014but often misunderstood in ways that have nothing to do with specific components.</p> <p>This section explores broad misconceptions that stem from surface-level understanding, outdated practices, or tribal knowledge. These general myths span topics like configuration, cluster setup, YAML practices, and default behaviors that many take for granted.</p> <p>By debunking these, you\u2019ll gain a stronger foundation for everything else Kubernetes has to offer\u2014making you a more confident and effective practitioner.</p>"},{"location":"general_myths/overview/#subtopics","title":"Subtopics","text":"<ul> <li>Myth 1: Kubernetes Namespaces Provide Complete Isolation </li> <li>Myth 2: Complete application can be rolled back in Kubernetes </li> <li>Myth 3: Kubernetes automatically roll back failed deployment</li> </ul>"},{"location":"networking_myths/Myth1_kube-proxy_assign_IP_address_to_Pods/","title":"Myth 1: kube-proxy assign IP address to Pods","text":"<p>A common belief among Kubernetes users is that kube-proxy is responsible for assigning IP addresses to pods. After all, it manages networking rules and enables communication between services\u2014so it must be the component handling pod IPs, right?</p>"},{"location":"networking_myths/Myth1_kube-proxy_assign_IP_address_to_Pods/#why-does-this-myth-exist","title":"Why Does This Myth Exist?","text":"<ol> <li>Kube-proxy\u2019s prominent role in networking: Kube-proxy is integral to service-level networking in Kubernetes. Since it manages traffic routing between services and uses iptables, many assume it also handles pod-level networking, including IP assignment.</li> <li>Confusion with iptables: Kube-proxy\u2019s involvement with iptables and traffic forwarding may create the impression that it manages all networking tasks, including assigning IPs to pods.</li> <li>Misunderstanding of Kubernetes components: Newcomers to Kubernetes often see kube-proxy running in the cluster and mistakenly believe it is responsible for all networking-related operations, including pod IP allocation.</li> </ol>"},{"location":"networking_myths/Myth1_kube-proxy_assign_IP_address_to_Pods/#the-reality","title":"The Reality","text":"<ul> <li>Pod IP allocation is the responsibility of the CNI (Container Network Interface) plugin, not kube-proxy. The CNI is responsible for assigning network interfaces and IP addresses to each pod when they are created.</li> <li>Kube-proxy\u2019s role is service-level networking: It ensures traffic reaches the right pods by managing network rules for services, but it does not assign IP addresses to the pods themselves.</li> <li>Pods depend on the CNI: If there\u2019s no CNI plugin installed or running, Kubernetes pods won\u2019t receive an IP address, even if kube-proxy is operational.</li> </ul>"},{"location":"networking_myths/Myth1_kube-proxy_assign_IP_address_to_Pods/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>To verify that kube-proxy is not responsible for pod IP allocation, follow these steps: Step1: Check the IP of an existing pod before disabling kube-proxy: Run the following command to view the pod\u2019s details:</p> <pre><code>kubectl get pods -o wide\n</code></pre> <p>Example output:</p> <pre><code>NAME          READY STATUS    IP          NODE\nnginx-abc123  1/1   Running  10.244.1.2  worker-node-1\n</code></pre> <p>Step2: Disable kube-proxy temporarily: Scale down the kube-proxy deployment to zero replicas:</p> <pre><code>kubectl scale deployment/kube-proxy -n kube-system --replicas=0\n</code></pre> <p>Step3: Create a new pod and check if it receives an IP address: Launch a new pod and Then, verify the new pod's details::</p> <pre><code>kubectl run test-pod --image=nginx --restart=Never\nkubectl get pods -o wide\n</code></pre> <p>You\u2019ll observe that the new pod still receives an IP address, even with kube-proxy disabled, which demonstrates that CNI, not kube-proxy, is responsible for assigning pod IPs.</p>"},{"location":"networking_myths/Myth1_kube-proxy_assign_IP_address_to_Pods/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kube-proxy does not assign pod IPs \u2013 Pod IP allocation is handled by the CNI plugin, not kube-proxy.</li> <li>Kube-proxy\u2019s role is service routing \u2013 It manages the traffic routing for services, but not the network configuration or IP assignment for pods.</li> <li>Without a CNI plugin, pods will fail to receive IP addresses, regardless of whether kube-proxy is running.</li> <li>Always ensure a CNI plugin is installed when setting up Kubernetes to guarantee proper networking functionality for pods.</li> </ul>"},{"location":"networking_myths/Myth2_ClusterIP_Service_is_Only_for_Internal_Traffic/","title":"Myth 2: ClusterIP Service is Only for Internal Traffic","text":"<p>You\u2019ve just configured a ClusterIP service to expose one of your pods internally in your Kubernetes cluster. It seems like a straightforward setup\u2014after all, ClusterIP services are meant to route traffic between pods within the cluster, right? So, you assume that it\u2019s all about internal communication.</p> <p>But then, you decide to test it a bit more. You try accessing the service using a curl command from a different pod within the same cluster and\u2026 it works perfectly. That\u2019s expected, right? You start thinking that ClusterIP is only for internal use. But curiosity leads you to try something else\u2014accessing the service from outside the cluster.</p> <p>To your surprise, it works! You use a load balancer or a proxy to access the service, and your request is successfully routed to the ClusterIP service. Wait\u2014how can this be? ClusterIP should only be for internal traffic, right?</p>"},{"location":"networking_myths/Myth2_ClusterIP_Service_is_Only_for_Internal_Traffic/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Kubernetes documentation emphasizes ClusterIP as an \"internal\" service type, leading people to overlook its role in external communication.</li> <li>NodePort and LoadBalancer are explicitly designed for external access, making it seem like ClusterIP plays no role in exposing services.</li> <li>Developers often misunderstand how Kubernetes routes traffic, missing the fact that external service types still forward traffic through ClusterIP.</li> </ol>"},{"location":"networking_myths/Myth2_ClusterIP_Service_is_Only_for_Internal_Traffic/#the-reality","title":"The Reality","text":"<p>Yes, a pure ClusterIP service is internal, but every Kubernetes service\u2014whether NodePort or LoadBalancer\u2014relies on ClusterIP behind the scenes. Even when traffic comes from outside the cluster, it still flows through ClusterIP before reaching the pods.</p>"},{"location":"networking_myths/Myth2_ClusterIP_Service_is_Only_for_Internal_Traffic/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a LoadBalancer Service Apply the following YAML to create a LoadBalancer service:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-loadbalancer-service\nspec:\n  selector:\n    app: my-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n  type: LoadBalancer\n</code></pre> <p>Step 2: Verify the Created Service Run the following command to check the details of the service:</p> <pre><code>kubectl get svc my-loadbalancer-service -o wide\n</code></pre> <p>Expected Output:</p> <pre><code>NAME                      TYPE           CLUSTER-IP   EXTERNAL-IP   PORT(S)       AGE\nmy-loadbalancer-service   LoadBalancer   10.96.0.150  &lt;pending&gt;     80:32000/TCP  5s\n</code></pre> <p>Notice the CLUSTER-IP field\u2014Kubernetes has automatically assigned a ClusterIP to the LoadBalancer service.</p> <p>Step 3: Confirm That ClusterIP Exists Even though the service is a LoadBalancer, it still functions like a ClusterIP service inside the cluster. You can verify this by running:</p> <pre><code>kubectl get endpoints my-loadbalancer-service\n</code></pre> <p>Expected Output:</p> <pre><code>NAME                      ENDPOINTS           AGE\nmy-loadbalancer-service   192.168.1.100:8080  5s\n</code></pre> <p>This proves that traffic is routed through the ClusterIP before reaching the pods.</p>"},{"location":"networking_myths/Myth2_ClusterIP_Service_is_Only_for_Internal_Traffic/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>ClusterIP is not just for internal traffic; it is essential for external services like NodePort and LoadBalancer.</li> <li>Every Kubernetes service type ultimately forwards traffic through ClusterIP before reaching the pods.</li> <li>Misunderstanding ClusterIP\u2019s role can lead to unnecessary service type changes and wasted troubleshooting efforts.</li> </ul>"},{"location":"networking_myths/Myth3_ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/","title":"Myth 3: ClusterIP Service Always Use Round-Robin Load Balancing","text":"<p>Have you ever assumed that your pods are getting equal traffic? Many engineers believe Kubernetes distributes traffic evenly across pods in a strict round-robin manner. But if you actually monitor request distribution, you\u2019ll notice something surprising\u2014some pods receive more traffic than others. Is Kubernetes failing at load balancing? Not really. It turns out that the default behavior isn't what most people expect.</p>"},{"location":"networking_myths/Myth3_ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>People assume Kubernetes works like traditional load balancers, which often default to round-robin.</li> <li>The presence of multiple backend pods creates an expectation of even traffic distribution.</li> <li>The internal working of kube-proxy is often overlooked</li> </ol>"},{"location":"networking_myths/Myth3_ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#the-reality","title":"The Reality","text":"<p>The default iptables-based kube-proxy does not use round-robin. Instead, it relies on random probability-based selection when forwarding traffic to backend pods.</p>"},{"location":"networking_myths/Myth3_ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a ClusterIP Service with Multiple Pods Apply the following YAML:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: test-clusterip\nspec:\n  selector:\n    app: test-app\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      labels:\n        app: test-app\n    spec:\n      containers:\n        - name: app\n          image: nginx\n          ports:\n            - containerPort: 8080\n\n</code></pre> <p>Step 1: Send Requests and Observe the Pattern Run multiple requests from a test pod:</p> <pre><code>kubectl run test --rm -it --image=busybox -- /bin/sh\n</code></pre> <p>Inside the pod, execute:</p> <pre><code>while true; do wget -qO- http://test-clusterip.default.svc.cluster.local; sleep 1; done\n</code></pre>"},{"location":"networking_myths/Myth3_ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#todo-include-kiali-image","title":"TODO - Include Kiali image","text":"<p>You'll notice that some pods receive more traffic than others\u2014proving that the selection is random and not round-robin.</p>"},{"location":"networking_myths/Myth3_ClusterIP_Services_Always_Use_Round-Robin_Load_Balancing/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>ClusterIP does not guarantee round-robin load balancing\u2014default iptables mode uses random selection.</li> <li>IPVS mode can enable true round-robin scheduling in Kubernetes.</li> <li>External load balancers provide more fine-grained control if round-robin is essential.</li> </ul>"},{"location":"networking_myths/Myth4_kubectl_port-forward_svc_sends_traffic_to_a_service/","title":"Myth 4: 'kubectl port-forward svc' sends traffic to a service","text":"<p>Many engineers assume that running:</p> <pre><code>kubectl port-forward svc/my-service 8080:80\n</code></pre> <p>routes traffic through the Service to its backend Pods\u2014just like a real client request would. But if you analyze the traffic flow, you'll notice something surprising\u2014the Service is completely bypassed!</p>"},{"location":"networking_myths/Myth4_kubectl_port-forward_svc_sends_traffic_to_a_service/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>The command uses <code>svc/my-service</code>, which makes it seem like traffic will flow through the Service.</li> <li>Since Services load balance traffic, it's natural to assume <code>port-forward</code> does the same.</li> <li>The actual forwarding logic is not well-documented, leading to confusion.</li> </ol>"},{"location":"networking_myths/Myth4_kubectl_port-forward_svc_sends_traffic_to_a_service/#the-reality","title":"The Reality","text":"<p>Even if you specify a Service in the <code>kubectl port-forward</code> command, the traffic never reaches the Service. Instead, Kubernetes picks a single Pod behind the Service and forwards traffic directly to that Pod\u2014bypassing the Service entirely.</p>"},{"location":"networking_myths/Myth4_kubectl_port-forward_svc_sends_traffic_to_a_service/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Step 1: Create a Service with Multiple Pods Apply the following YAML:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: productpage\nspec:\n  selector:\n    app: productpage\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 8080\n\n</code></pre> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: productpage\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: productpage\n  template:\n    metadata:\n      labels:\n        app: productpage\n    spec:\n      containers:\n        - name: app\n          image: nginx\n          ports:\n            - containerPort: 8080\n\n</code></pre> <p>Step 2: Run kubectl port-forward and Check Traffic Flow Forward traffic using:</p> <pre><code>kubectl port-forward svc/productpage 8080:80\n</code></pre> <p>Then check which Pod is actually receiving traffic:</p>"},{"location":"networking_myths/Myth4_kubectl_port-forward_svc_sends_traffic_to_a_service/#todo-include-kiali-image","title":"TODO - Include Kiali image","text":"<p>You'll notice no traffic hits the Service itself, confirming that requests go directly to a Pod.</p>"},{"location":"networking_myths/Myth4_kubectl_port-forward_svc_sends_traffic_to_a_service/#key-takeaways","title":"Key Takeaways","text":"<ul> <li><code>kubectl port-forward</code> does not send traffic through the Service\u2014it picks one Pod and forwards directly.</li> <li>No load balancing: Traffic is not distributed among multiple Pods.</li> <li>No failover: If the chosen Pod crashes, the connection is lost.</li> <li>Not a true service test: If you want to simulate real client traffic, use Ingress, LoadBalancer, or NodePort.</li> </ul>"},{"location":"networking_myths/overview/","title":"Kubernetes Networking Myths Overview","text":"<p>Networking in Kubernetes often feels like magic\u2014until it breaks.</p> <p>This section uncovers common misconceptions about how networking works inside a cluster. From Pod-to-Pod communication and Services to DNS, Network Policies, and CNI plugins, these myths reveal gaps that lead to confusing outages, misconfigured policies, and insecure setups.</p> <p>Whether you're debugging traffic issues or designing multi-tenant clusters, busting these myths will sharpen your understanding of Kubernetes networking internals.</p>"},{"location":"networking_myths/overview/#subtopics","title":"Subtopics","text":"<ul> <li>Myth 1: kube-proxy assign IP address to Pods </li> <li>Myth 2: ClusterIP Service is Only for Internal Traffic </li> <li>Myth 3: ClusterIP Service Always Use Round-Robin Load Balancing</li> <li>Myth 4: 'kubectl port-forward svc' sends traffic to a service</li> </ul>"},{"location":"pod_myths/overview/","title":"Work in progress","text":""},{"location":"workload_myths/Myth1_Rolling_Updates_Are_Only_Supported_by_Deployments/","title":"Myth 1: Rolling Updates Are Only Supported by Deployments","text":"<p>Many believe that rolling updates are an exclusive feature of Kubernetes Deployments. This assumption leads to the misconception that StatefulSets and DaemonSets do not support rolling updates, forcing teams to use workarounds. But is this really the case?</p>"},{"location":"workload_myths/Myth1_Rolling_Updates_Are_Only_Supported_by_Deployments/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Deployments are widely documented for rolling updates, making them the most well-known approach.</li> <li>Lack of awareness about update strategies in other controllers like StatefulSets and DaemonSets.</li> <li>Confusion around update behavior\u2014each controller has a different approach to handling updates.</li> </ol>"},{"location":"workload_myths/Myth1_Rolling_Updates_Are_Only_Supported_by_Deployments/#the-reality","title":"The Reality:","text":"<p>Rolling Updates Extend Beyond Deployments. While Deployments use the default RollingUpdate strategy, StatefulSets and DaemonSets also support rolling updates\u2014but with different behaviors. - Deployments: Roll updates across Pods gradually using RollingUpdate strategy. - StatefulSets: Follow a rolling update pattern but update one Pod at a time in order. - DaemonSets: Perform rolling updates but have different scheduling constraints, ensuring Pods are only running on specific nodes.</p>"},{"location":"workload_myths/Myth1_Rolling_Updates_Are_Only_Supported_by_Deployments/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Let\u2019s look at a rolling update example for each controller.</p>"},{"location":"workload_myths/Myth1_Rolling_Updates_Are_Only_Supported_by_Deployments/#todo-need-to-add-proper-example","title":"TODO - Need to add proper example","text":""},{"location":"workload_myths/Myth1_Rolling_Updates_Are_Only_Supported_by_Deployments/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Rolling updates are not exclusive to Deployments\u2014StatefulSets and DaemonSets also support them.</li> <li>Each controller has a different update pattern\u2014understanding them prevents unexpected behavior.</li> <li>Kubernetes ensures updates happen safely\u2014but choosing the right approach matters!</li> </ul>"},{"location":"workload_myths/Myth2_DaemonSet_always_schedule_pods_on_all_nodes/","title":"Myth 2: DaemonSet always schedule pods on all nodes.","text":"<p>Expect DaemonSet to Run on Every Node? Not So Fast! Many assume that a DaemonSet automatically schedules a Pod on every node in the cluster. While this is generally true, there are several cases where DaemonSet does not schedule Pods on all nodes.</p>"},{"location":"workload_myths/Myth2_DaemonSet_always_schedule_pods_on_all_nodes/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>The default behavior of a DaemonSet is to run a Pod on each node, leading to the assumption that it applies universally.</li> <li>Many Kubernetes tutorials demonstrate cluster-wide scheduling without mentioning constraints.</li> <li>The impact of node taints, selectors, and affinity rules is often overlooked.</li> </ol>"},{"location":"workload_myths/Myth2_DaemonSet_always_schedule_pods_on_all_nodes/#the-reality","title":"The Reality:","text":"<p>DaemonSet Scheduling Is Conditional. It does not always schedule Pods on every node. Various factors can prevent or control where Pods are scheduled: - Taints and Tolerations \u2013 Nodes with taints will reject DaemonSet Pods unless explicitly tolerated. - Node Selectors \u2013 DaemonSet Pods are only scheduled on nodes matching the specified labels. - Affinity and Anti-Affinity Rules \u2013 Custom scheduling rules can limit DaemonSet Pod placement.</p>"},{"location":"workload_myths/Myth2_DaemonSet_always_schedule_pods_on_all_nodes/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Let\u2019s see a example where DaemonSet does NOT schedule Pods on all nodes: Node Selector: DaemonSet Pods Only on Labeled Node</p> <pre><code>apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: my-daemonset\nspec:\n  template:\n    spec:\n      nodeSelector:\n        custom-role: worker-node\n      containers:\n      - name: my-daemon\n        image: my-app:v2\n</code></pre> <p>Here, Pods will only run on nodes labeled <code>custom-role=worker-node</code>, skipping all others.</p>"},{"location":"workload_myths/Myth2_DaemonSet_always_schedule_pods_on_all_nodes/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>DaemonSets do not always schedule on all nodes\u2014constraints like taints, selectors, and affinity rules affect placement.</li> <li>By default, they schedule on worker nodes but require tolerations for control-plane nodes.</li> <li>Understanding scheduling logic is crucial to ensure Pods run where they are needed.</li> </ul>"},{"location":"workload_myths/Myth3_Deployment_Supports_All_Pod_Restart_Policies/","title":"Myth 3: Deployment Supports All Pod Restart Policies","text":"<p>Can You Use Any Restart Policy in a Deployment?  Many believe that Deployments can be used with any Kubernetes Pod restart policy. The assumption is that you can define <code>Always</code>, <code>OnFailure</code>, or <code>Never</code> as the restartPolicy within a Deployment\u2019s Pod spec.But that's not true.</p>"},{"location":"workload_myths/Myth3_Deployment_Supports_All_Pod_Restart_Policies/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li>Pods support all three restart policies (<code>Always</code>, <code>OnFailure</code>, <code>Never</code>), leading to the assumption that Deployments do as well.</li> <li>Users often assume Pod behavior is the same inside different controllers like Deployments, DaemonSets, or Jobs.</li> <li>The Kubernetes API allows specifying other restart policies in Pod definitions, but Deployments override them!</li> </ol>"},{"location":"workload_myths/Myth3_Deployment_Supports_All_Pod_Restart_Policies/#the-reality","title":"The Reality:","text":"<p>Deployment = <code>restartPolicy: Always</code> Only! In Kubernetes, Deployments are designed to manage long-running applications. That means: - If a Pod fails, Kubernetes restarts it. - If a node crashes, a new Pod is scheduled on another node. - Any restart policy other than Always is rejected!</p>"},{"location":"workload_myths/Myth3_Deployment_Supports_All_Pod_Restart_Policies/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>Let\u2019s try using <code>restartPolicy: OnFailure</code> inside a Deployment and see what happens:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: test-app\n  template:\n    metadata:\n      labels:\n        app: test-app\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: test-container\n        image: busybox\n        command: [\"sh\", \"-c\", \"echo Hello &amp;&amp; exit 1\"]\n</code></pre> <p>What Happens? Deployment rejects <code>OnFailure</code> and throw an error!</p> <pre><code>The Deployment \"my-deployment\" is invalid: spec.template.spec.restartPolicy: Unsupported value: \"OnFailure\": supported values: \"Always\"\n</code></pre> <p>Kubernetes rejects the Deployment, as it does not allow restart policies other than <code>Always</code>.</p>"},{"location":"workload_myths/Myth3_Deployment_Supports_All_Pod_Restart_Policies/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Deployments only support <code>restartPolicy: Always</code>\u2014other policies are rejected.</li> <li>Use Jobs or CronJobs for <code>OnFailure</code> or <code>Never</code> restart behavior.</li> <li>Understanding controller-specific behavior prevents misconfigurations.</li> </ul>"},{"location":"workload_myths/Myth4_Deployments_Automatically_Roll_Back_on_Failure/","title":"Myth 4: Deployment Automatically Roll Back on Failure","text":"<p>Have you ever assumed that a failed Deployment would automatically roll back to the last working version? Many believe that Kubernetes automatically detects a failed rollout and reverts to the previous version. However, this is not entirely true.</p>"},{"location":"workload_myths/Myth4_Deployments_Automatically_Roll_Back_on_Failure/#why-this-myth-exists","title":"Why This Myth Exists?","text":"<ol> <li><code>kubectl rollout undo</code> exists, leading people to assume rollbacks are automatic.</li> <li>Some CI/CD tools implement automatic rollback logic, making it seem like Kubernetes does it by default.</li> <li>Kubernetes pauses a failed rollout but does not revert the change automatically.</li> </ol>"},{"location":"workload_myths/Myth4_Deployments_Automatically_Roll_Back_on_Failure/#the-reality","title":"The Reality:","text":"<p>By default, Kubernetes does not automatically rollback a failed Deployment. Instead, it: - Pauses the rollout if a newly updated pod crashes or fails health checks. - Keeps old running pods unchanged, preventing further failures. - Requires manual intervention to rollback or fix the issue.</p>"},{"location":"workload_myths/Myth4_Deployments_Automatically_Roll_Back_on_Failure/#experiment-validate","title":"Experiment &amp; Validate","text":"<p>In Progress</p>"},{"location":"workload_myths/Myth4_Deployments_Automatically_Roll_Back_on_Failure/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Kubernetes does not auto-rollback failed Deployments.</li> <li>It pauses rollouts, but you must manually trigger a rollback.</li> <li>Use health checks and CI/CD automation to enable safer rollbacks.</li> </ul>"},{"location":"workload_myths/overview/","title":"Kubernetes Workload Myths Overview","text":"<p>Deploying workloads in Kubernetes may seem straightforward\u2014but lurking underneath are subtle behaviors that often surprise even experienced engineers.</p> <p>This section dives into common misconceptions around Pods, Deployments, StatefulSets, Jobs, and DaemonSets. Misunderstanding how these controllers behave can lead to reliability issues, scaling problems, and unexpected restarts in production.</p> <p>By exposing these myths, we help you design resilient, predictable workloads that behave as intended under real-world pressure.</p>"},{"location":"workload_myths/overview/#subtopics","title":"Subtopics","text":"<ul> <li>Myth 1: Rolling Updates Are Only Supported by Deployments </li> <li>Myth 2: DaemonSet always schedule pods on all nodes </li> <li>Myth 3: Deployment Supports All Pod Restart Policies</li> <li>Myth 4: Deployment Automatically Roll Back on Failure</li> </ul>"}]}